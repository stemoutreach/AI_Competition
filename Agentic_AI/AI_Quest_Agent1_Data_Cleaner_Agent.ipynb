{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0368c1",
   "metadata": {},
   "source": [
    "# AI Quest — Agentic AI Problem 1  \n",
    "**Title:** Single‑Agent Data Cleaner & Modeler (with Human‑in‑the‑Loop)\n",
    "\n",
    "## Scenario\n",
    "Design a **single agent** that chooses from available **tools** to clean data, engineer features, train a model, and meet **accuracy and fairness** targets. The agent must log actions (transparency) and request **human approval** before executing its plan.\n",
    "\n",
    "## Tools (provided as Python functions)\n",
    "- `preview_data()`, `impute_numeric()`, `encode_categorical()`, `scale_numeric()`\n",
    "- `train(algorithm)`, `evaluate()`, `check_fairness()`\n",
    "- `set_thresholds(th_A, th_B)` (group-aware mitigation)\n",
    "\n",
    "## Objectives\n",
    "- Implement an agent loop: **plan → (human approve) → act → evaluate → iterate**\n",
    "- Achieve target metrics; keep a **transparent action log**\n",
    "- Include a **stop condition** and **fallback policy**\n",
    "\n",
    "## Deliverables\n",
    "- Working notebook with agent class, action log, final metrics, and fairness\n",
    "- Short Markdown: explain **why** the final plan is acceptable (accountability + human agency)\n",
    "\n",
    "## Scoring (auto-checked in notebook)\n",
    "- Accuracy ≥ 0.78 and F1 ≥ 0.78 (30 pts)\n",
    "- Equal Opportunity |Δ| ≤ 0.15 after mitigation (30 pts)\n",
    "- Action log contains ≥ 6 steps incl. human approval checkpoint (20 pts)\n",
    "- Markdown rationale provided (20 pts)\n",
    "\n",
    "**Total:** 100 pts\n",
    "\n",
    "## Ethics\n",
    "- **Transparency:** action log\n",
    "- **Explainability:** report feature importance\n",
    "- **Fairness:** parity/opportunity checks + mitigation\n",
    "- **Human Agency:** explicit approval gate to proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ea374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional installs\n",
    "# !pip -q install pandas numpy scikit-learn matplotlib\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "RANDOM_SEED = 11\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(\"✅ Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bab5eb",
   "metadata": {},
   "source": [
    "## 1) Data generation (with embedded bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753ba8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3000\n",
    "x1 = np.random.normal(0,1,size=n)\n",
    "x2 = np.random.normal(1,1.2,size=n)\n",
    "x3 = np.random.normal(-0.5,0.8,size=n)\n",
    "group = np.random.choice([\"A\",\"B\"], p=[0.5,0.5], size=n)\n",
    "\n",
    "base = 0.5 + 0.4*x1 - 0.3*x2 + 0.2*x3\n",
    "bias = np.where(group==\"B\",-0.08,0.0)\n",
    "p = 1/(1+np.exp(-(base+bias)))\n",
    "y = (np.random.rand(n) < p).astype(int)\n",
    "\n",
    "df = pd.DataFrame({\"x1\":x1,\"x2\":x2,\"x3\":x3,\"group\":group,\"y\":y})\n",
    "# add 3% missingness\n",
    "for c in [\"x1\",\"x2\",\"x3\"]: df.loc[np.random.choice(n, int(0.03*n), replace=False), c] = np.nan\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca8043",
   "metadata": {},
   "source": [
    "## 2) Tools library (callable by the agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7084c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\"df\": None, \"X\": None, \"y\": None, \"prot\": None, \"scaler\": None, \"model\": None, \"thresholds\": {\"A\":0.5,\"B\":0.5}}\n",
    "state[\"df\"] = df.copy()\n",
    "\n",
    "def logprint(log, msg): \n",
    "    print(msg); log.append(msg)\n",
    "\n",
    "def preview_data(log):\n",
    "    logprint(log, f\"Preview: {state['df'].head(2)}\")\n",
    "\n",
    "def impute_numeric(log):\n",
    "    logprint(log, \"Imputing missing numeric values (median)\")\n",
    "    num_cols = [\"x1\",\"x2\",\"x3\"]\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    state[\"df\"][num_cols] = imp.fit_transform(state[\"df\"][num_cols])\n",
    "\n",
    "def encode_categorical(log):\n",
    "    logprint(log, \"Encoding 'group' to indicator (group_B)\")\n",
    "    state[\"df\"][\"group_B\"] = (state[\"df\"][\"group\"]==\"B\").astype(int)\n",
    "\n",
    "def scale_numeric(log):\n",
    "    logprint(log, \"Standard scaling numeric features\")\n",
    "    num_cols = [\"x1\",\"x2\",\"x3\"]\n",
    "    sc = StandardScaler()\n",
    "    state[\"df\"][num_cols] = sc.fit_transform(state[\"df\"][num_cols])\n",
    "    state[\"scaler\"] = sc\n",
    "\n",
    "def split_xy(log):\n",
    "    logprint(log, \"Splitting features/labels and train/test\")\n",
    "    X = state[\"df\"][[\"x1\",\"x2\",\"x3\",\"group_B\"]].copy()\n",
    "    y = state[\"df\"][\"y\"].values\n",
    "    prot = state[\"df\"][\"group\"].values\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    Xtr, Xte, ytr, yte, ptr, pte = train_test_split(X, y, prot, test_size=0.25, random_state=RANDOM_SEED, stratify=y)\n",
    "    state[\"X\"]=(Xtr,Xte); state[\"y\"]=(ytr,yte); state[\"prot\"]=(ptr,pte)\n",
    "\n",
    "def train_model(log, algorithm=\"logreg\"):\n",
    "    logprint(log, f\"Training model: {algorithm}\")\n",
    "    Xtr, Xte = state[\"X\"]\n",
    "    ytr, yte = state[\"y\"]\n",
    "    if algorithm==\"logreg\":\n",
    "        m = LogisticRegression(max_iter=200)\n",
    "    else:\n",
    "        m = RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=200, max_depth=None)\n",
    "    m.fit(Xtr, ytr)\n",
    "    state[\"model\"]=m\n",
    "\n",
    "def evaluate(log, thresholds=None):\n",
    "    Xtr, Xte = state[\"X\"]; ytr, yte = state[\"y\"]; ptr, pte = state[\"prot\"]\n",
    "    m = state[\"model\"]\n",
    "    proba = m.predict_proba(Xte)[:,1] if hasattr(m,\"predict_proba\") else m.predict(Xte)\n",
    "    thA = thresholds.get(\"A\",0.5) if thresholds else state[\"thresholds\"][\"A\"]\n",
    "    thB = thresholds.get(\"B\",0.5) if thresholds else state[\"thresholds\"][\"B\"]\n",
    "    A = (pte==\"A\"); B=(pte==\"B\")\n",
    "    yhat = np.zeros_like(yte); yhat[A] = (proba[A]>=thA).astype(int); yhat[B]=(proba[B]>=thB).astype(int)\n",
    "    metrics = {\"accuracy\": accuracy_score(yte,yhat), \"f1\": f1_score(yte,yhat), \"precision\": precision_score(yte,yhat), \"recall\": recall_score(yte,yhat)}\n",
    "    return metrics, proba, yhat, pte, yte\n",
    "\n",
    "def check_fairness(log, y_true, y_pred, prot):\n",
    "    A = (prot==\"A\"); B=(prot==\"B\")\n",
    "    pA = y_pred[A].mean(); pB = y_pred[B].mean()\n",
    "    dp = pA - pB\n",
    "    def tpr(mask):\n",
    "        if (y_true[mask]==1).sum()==0: return np.nan\n",
    "        return ((y_pred[mask]==1)&(y_true[mask]==1)).sum()/(y_true[mask]==1).sum()\n",
    "    eod = tpr(A)-tpr(B)\n",
    "    fm = {\"demographic_parity_diff\": float(dp), \"equal_opportunity_diff\": float(eod)}\n",
    "    logprint(log, f\"Fairness: {fm}\")\n",
    "    return fm\n",
    "\n",
    "def set_thresholds(log, th_A, th_B):\n",
    "    logprint(log, f\"Setting thresholds A={th_A:.2f}, B={th_B:.2f}\")\n",
    "    state[\"thresholds\"][\"A\"]=th_A; state[\"thresholds\"][\"B\"]=th_B\n",
    "\n",
    "def feature_importance(log):\n",
    "    logprint(log, \"Permutation importance for explainability\")\n",
    "    Xtr, Xte = state[\"X\"]; ytr, yte = state[\"y\"]\n",
    "    m = state[\"model\"]\n",
    "    r = permutation_importance(m, Xte, yte, scoring=\"f1\", n_repeats=5, random_state=RANDOM_SEED)\n",
    "    imp = pd.DataFrame({\"feature\": Xte.columns, \"importance\": r.importances_mean}).sort_values(\"importance\", ascending=False)\n",
    "    logprint(log, f\"Top features:\\n{imp.head(5)}\")\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e841e5",
   "metadata": {},
   "source": [
    "## 3) Agent loop (plan → human approve → act → evaluate → iterate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f720f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    def __init__(self):\n",
    "        self.log = []\n",
    "        self.steps = 0\n",
    "\n",
    "    def propose_plan(self):\n",
    "        # A minimal fixed plan; you may make this adaptive\n",
    "        plan = [\n",
    "            (\"preview_data\", {}),\n",
    "            (\"impute_numeric\", {}),\n",
    "            (\"encode_categorical\", {}),\n",
    "            (\"scale_numeric\", {}),\n",
    "            (\"split_xy\", {}),\n",
    "            (\"train_model\", {\"algorithm\":\"rf\"}),\n",
    "            (\"evaluate\", {}),\n",
    "            (\"check_fairness\", {}),\n",
    "            (\"set_thresholds\", {\"th_A\":0.50,\"th_B\":0.46}),\n",
    "            (\"evaluate\", {}),\n",
    "            (\"feature_importance\", {}),\n",
    "        ]\n",
    "        return plan\n",
    "\n",
    "    def human_approve(self, plan):\n",
    "        # In live event, ask a human; here we simulate approval\n",
    "        logprint(self.log, f\"Requesting human approval for plan of {len(plan)} steps... APPROVED\")\n",
    "        return True\n",
    "\n",
    "    def run(self):\n",
    "        plan = self.propose_plan()\n",
    "        if not self.human_approve(plan):\n",
    "            logprint(self.log, \"Plan rejected by human. Exiting.\")\n",
    "            return None\n",
    "        last_metrics=None; last_fair=None\n",
    "        for action, params in plan:\n",
    "            self.steps += 1\n",
    "            if action==\"preview_data\": preview_data(self.log)\n",
    "            elif action==\"impute_numeric\": impute_numeric(self.log)\n",
    "            elif action==\"encode_categorical\": encode_categorical(self.log)\n",
    "            elif action==\"scale_numeric\": scale_numeric(self.log)\n",
    "            elif action==\"split_xy\": split_xy(self.log)\n",
    "            elif action==\"train_model\": train_model(self.log, **params)\n",
    "            elif action==\"evaluate\":\n",
    "                m, proba, yhat, prot, ytrue = evaluate(self.log)\n",
    "                last_metrics=m; last_proba=proba; last_yhat=yhat; last_prot=prot; last_ytrue=ytrue\n",
    "                logprint(self.log, f\"Metrics: {m}\")\n",
    "            elif action==\"check_fairness\":\n",
    "                last_fair = check_fairness(self.log, last_ytrue, last_yhat, last_prot)\n",
    "            elif action==\"set_thresholds\":\n",
    "                set_thresholds(self.log, **params)\n",
    "            elif action==\"feature_importance\":\n",
    "                feature_importance(self.log)\n",
    "        return {\"metrics\": last_metrics, \"fairness\": last_fair, \"log\": self.log}\n",
    "\n",
    "agent = SimpleAgent()\n",
    "result = agent.run()\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dead8b",
   "metadata": {},
   "source": [
    "## 4) Accountability & Human Agency (Markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your rationale in a Markdown cell outside of code in the notebook.\n",
    "rationale_claim = True  # set to True after you add your explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b297ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Auto-Scoring ===\n",
    "score = 0\n",
    "m = result[\"metrics\"]; f = result[\"fairness\"]; steps = len(result[\"log\"])\n",
    "\n",
    "if m[\"accuracy\"]>=0.78 and m[\"f1\"]>=0.78: score+=30\n",
    "if abs(f[\"equal_opportunity_diff\"])<=0.15: score+=30\n",
    "if steps>=6: score+=20\n",
    "if rationale_claim: score+=20\n",
    "\n",
    "summary = {\"accuracy\": m[\"accuracy\"], \"f1\": m[\"f1\"], \"eod\": f[\"equal_opportunity_diff\"], \"steps\": steps, \"score\": score}\n",
    "print(\"✅ Final Summary:\", summary)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
